<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0,  viewport-fit=cover" name="viewport" />
    <meta name="description" content="POSE" />
    <meta name="hexo-theme-A4" content="v1.6.9" />
    <link rel="alternate icon" type="image/webp" href="/img/favicon.webp">
    <title>hwan | </title>

    
        
            
<link rel="stylesheet" href="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/css/reset.css">

            
<link rel="stylesheet" href="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/css/markdown.css">

            
<link rel="stylesheet" href="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/css/fonts.css">
 
            <!--注意：首页既不是post也不是page-->
            
        
    
    
<link rel="stylesheet" href="/css/ui.css">
 
    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>
    
    <body>
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    
<div class="header">
    <div class="header-container">
        <img style="
        width: 56px;
        height: auto;" alt="^-^" cache-control="max-age=86400" class="header-img" src="/img/favicon.webp" width="10%"></img>
        <div class="header-content">
            <a class="logo" href="/">hwan</a> 
            <span class="description"></span> 
        </div>
        
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">首页</a></li>
            
        
            
                <li><a href="/list/">文章</a></li>
            
        
            
                <li><a href="/about/">关于</a></li>
            
        
            
                <li><a href="/tags/">标签</a></li>
            
        
            
                <li><a href="/categories/">分类</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--说明是文章post页面-->
                    
                        <div class="post-main">

    
        <div class="post-main-title">
            POSE
        </div>
      
    

    <div class="post-md">
        
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#RLE"><span class="post-toc-text">RLE</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E4%B8%8E%E5%A7%BF%E6%80%81%E7%9B%B8%E5%85%B3%E7%9A%84%E8%AE%BA%E6%96%87%E4%B8%BB%E8%A6%81%E7%9C%8B%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84"><span class="post-toc-text">与姿态相关的论文主要看数据组织结构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#DAS"><span class="post-toc-text">DAS</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#voxel-pose"><span class="post-toc-text">voxel_pose</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#PCT"><span class="post-toc-text">PCT</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#GFPose"><span class="post-toc-text">GFPose</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E5%A7%BF%E6%80%81%E7%9B%B8%E5%85%B3%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="post-toc-text">姿态相关的数据</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#CrowdPose"><span class="post-toc-text">CrowdPose</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#OCHuman"><span class="post-toc-text">OCHuman</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#SyncOCC"><span class="post-toc-text">SyncOCC</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#MPII"><span class="post-toc-text">MPII</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#CMU-Panoptic"><span class="post-toc-text">CMU Panoptic</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="post-toc-text">使用方法</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#COCO-keypoint%E6%95%B0%E6%8D%AE"><span class="post-toc-text">COCO keypoint数据</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#MuPoTS"><span class="post-toc-text">MuPoTS</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#MuCo"><span class="post-toc-text">MuCo</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Human-3-6%E4%BD%BF%E7%94%A8"><span class="post-toc-text">Human 3.6使用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#POSENET%E7%BB%84%E7%BB%87%E6%A0%BC%E5%BC%8F"><span class="post-toc-text">POSENET组织格式</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#RLE%E7%BB%84%E7%BB%87%E6%A0%BC%E5%BC%8F"><span class="post-toc-text">RLE组织格式</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E5%AD%A6%E4%B9%A0RLE%E4%BB%A3%E7%A0%81"><span class="post-toc-text">学习RLE代码</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Train-on-MSCOCO"><span class="post-toc-text">Train on MSCOCO</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%86%99%E4%BB%A3%E7%A0%81%E7%9A%84%E6%8A%80%E5%B7%A7"><span class="post-toc-text">写代码的技巧</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#opt-snapshot"><span class="post-toc-text">opt.snapshot</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E2%80%98pytorch%E2%80%99-%E2%80%98slurm%E2%80%99-%E2%80%98mpi%E2%80%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0"><span class="post-toc-text">‘pytorch’, ‘slurm’, ‘mpi’分布式平台</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%97%A5%E5%BF%97%E5%87%BD%E6%95%B0"><span class="post-toc-text">日志函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90%E8%AE%BE%E7%BD%AE"><span class="post-toc-text">随机种子设置</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GPU"><span class="post-toc-text">GPU</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#%E7%BB%9F%E8%AE%A1GPU%E4%B8%AA%E6%95%B0"><span class="post-toc-text">统计GPU个数</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%B9%B6%E8%A1%8C%E8%BF%90%E7%AE%97"><span class="post-toc-text">多进程并行运算</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="post-toc-text">代码</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97"><span class="post-toc-text">网络模块</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SPPE"><span class="post-toc-text">SPPE</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#RegressFlow"><span class="post-toc-text">RegressFlow</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#RegressFlow3D"><span class="post-toc-text">RegressFlow3D</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
        
        <h1 id="RLE"><a href="#RLE" class="headerlink" title="RLE"></a>RLE</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/625023463">https://zhuanlan.zhihu.com/p/625023463</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/440567782">https://zhuanlan.zhihu.com/p/440567782</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/429017412">https://zhuanlan.zhihu.com/p/429017412</a> 讲的最好的文章</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Jeff-sjtu/res-loglikelihood-regression">https://github.com/Jeff-sjtu/res-loglikelihood-regression</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV18E411w7Jh/?spm_id_from=333.999.0.0">https://www.bilibili.com/video/BV18E411w7Jh/?spm_id_from=333.999.0.0</a> 讲flow的视频</p>
<p>GAN训练：模式崩溃和后置崩溃挑战</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/165577850">https://zhuanlan.zhihu.com/p/165577850</a> flow的文章</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59615785">https://zhuanlan.zhihu.com/p/59615785</a> 讲flow的文章</p>
<pre><code>20个采样点 10个实列
all number of params: 43108693
number of backbone params: 33841352
</code></pre>
<pre><code>20个采样点 5个实列
all number of params: 43105908
number of backbone params: 33841352
</code></pre>
<pre><code>10个采样点 5个实列
all number of params: 42365748
number of backbone params: 33841352
</code></pre>
<p>19416917</p>
<p>尺寸不变</p>
<pre><code>kernel_size=3, padding=1, stride=1 
kernel_size=5, padding=2, stride=1
kernel_size=7, padding=3, stride=1
</code></pre>
<h1 id="与姿态相关的论文主要看数据组织结构"><a href="#与姿态相关的论文主要看数据组织结构" class="headerlink" title="与姿态相关的论文主要看数据组织结构"></a>与姿态相关的论文主要看数据组织结构</h1><h2 id="DAS"><a href="#DAS" class="headerlink" title="DAS"></a>DAS</h2><pre><code>$&#123;ROOT&#125;
|-- data
|   |-- panoptic
|   |   |-- 160226_haggling1
|   |   |   |-- hdImgs
|   |   |   |   |-- 00_16
|   |   |   |   |-- 00_30
|   |   |-- 160422_haggling1
|   |   |-- 160226_mafia1
|   |   |-- 160422_mafia2
|   |   |-- 160226_ultimatum1
|   |   |-- 160422_ultimatum1
|   |   |-- 160906_pizza1
|   |   |-- annotations
|   |   |   |-- train.json
|   |   |   |-- haggling.json
|   |   |   |-- mafia.json
|   |   |   |-- ultimatum.json
|   |   |   |-- pizza.json
|   |-- coco
|   |   |-- train2017
|   |   |-- annotations
|   |   |   |-- person_keypoints_train2017.json
|   |-- muco
|   |   |-- unaugmented_set
|   |   |-- augmented_set
|   |   |-- annotations
|   |   |   |-- train_all_interv1.json
|   |-- mupots
|   |   |-- TS1
|   |   |   |-- img_000000.jpg
|   |   |   |-- ...
|   |   |-- ...
|   |   |-- TS20
|   |   |-- annotations
|   |   |   |-- MuPoTS-3D.json
</code></pre>
<h2 id="voxel-pose"><a href="#voxel-pose" class="headerlink" title="voxel_pose"></a>voxel_pose</h2><pre><code>$&#123;POSE_ROOT&#125;
|-- models
|   |-- pose_resnet50_panoptic.pth.tar
|-- data
    |-- panoptic-toolbox
        |-- data
            |-- 16060224_haggling1
            |   |-- hdImgs
            |   |-- hdvideos
            |   |-- hdPose3d_stage1_coco19
            |   |-- calibration_160224_haggling1.json
            |-- 160226_haggling1  
            |-- ...
</code></pre>
<h2 id="PCT"><a href="#PCT" class="headerlink" title="PCT"></a>PCT</h2><pre><code>$&#123;POSE_ROOT&#125;
|-- data
`-- |-- coco
    `-- |-- annotations
        |   |-- person_keypoints_train2017.json
        |   `-- person_keypoints_val2017.json
        |-- person_detection_results
        |   |-- COCO_val2017_detections_AP_H_56_person.json
        |   |-- COCO_test-dev2017_detections_AP_H_609_person.json
        `-- images
            |-- train2017
            |   |-- 000000000009.jpg
            |   |-- 000000000025.jpg
            |   |-- 000000000030.jpg
            |   |-- ... 
            `-- val2017
                |-- 000000000139.jpg
                |-- 000000000285.jpg
                |-- 000000000632.jpg
                |-- ... 
</code></pre>
<h2 id="GFPose"><a href="#GFPose" class="headerlink" title="GFPose"></a>GFPose</h2><pre><code>$&#123;POSE_ROOT&#125;
|-- configs
|-- lib
|-- run
|-- checkpoint
    |-- u3d
        |-- best_model.pth
|-- data
    |-- h36m
        |-- h36m_train.pkl
        |-- h36m_test.pkl
        |-- h36m_sh_dt_ft.pkl
</code></pre>
<h1 id="姿态相关的数据"><a href="#姿态相关的数据" class="headerlink" title="姿态相关的数据"></a>姿态相关的数据</h1><h2 id="CrowdPose"><a href="#CrowdPose" class="headerlink" title="CrowdPose"></a>CrowdPose</h2><h2 id="OCHuman"><a href="#OCHuman" class="headerlink" title="OCHuman"></a>OCHuman</h2><h2 id="SyncOCC"><a href="#SyncOCC" class="headerlink" title="SyncOCC"></a>SyncOCC</h2><p>合成的数据</p>
<h2 id="MPII"><a href="#MPII" class="headerlink" title="MPII"></a>MPII</h2><p>The dataset includes around <strong>25K images</strong> containing over <strong>40K people</strong> with annotated body joints；</p>
<p><strong>410 human activities</strong> and each image is provided with an activity label；</p>
<p>POSENET也有下载链接，组织为</p>
<pre><code>|   |-- MPII
|   |   |-- images
|   |   |-- annotations
</code></pre>
<p>官网也有</p>
<p><a target="_blank" rel="noopener" href="http://human-pose.mpi-inf.mpg.de/#download">http://human-pose.mpi-inf.mpg.de/#download</a></p>
<h2 id="CMU-Panoptic"><a href="#CMU-Panoptic" class="headerlink" title="CMU Panoptic"></a>CMU Panoptic</h2><p><a target="_blank" rel="noopener" href="http://domedb.perception.cs.cmu.edu/">http://domedb.perception.cs.cmu.edu/</a> 数据主页</p>
<p><a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/panoptic-toolbox">https://github.com/CMU-Perceptual-Computing-Lab/panoptic-toolbox</a></p>
<p>31个 多视角HD video（高清视频）通常指 1080p（1920x1080 像素）或更高分辨率的视频。</p>
<h6 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h6><p>下载这个PanopticStudio Toolbox，然后</p>
<pre><code>./scripts/getData.sh 171204_pose1_sample 举列子
./scripts/getData.sh 171204_pose1

./scripts/getData.sh (sequenceName) (VGA_Video_Number) (HD_Video_Number)
列如./scripts/getData.sh 171204_pose1_sample 240 10 
240 vga videos and 10 videos.

可选的序列名称
https://docs.google.com/spreadsheets/d/1eoe74dHRtoMVVFLKCTJkAtF8zqxAnoo2Nt15CYYvHEE/edit#gid=1333444170

./scripts/getDB_panopticHD_ver1_2.sh
下载1.2数据版本所有数据
上述命令默认是不下载视频数据的，就要像上面一样指定视频个数

假如说现在下载好了视频，需要将视频变成每一帧每一帧的，然后将一视频的标签也变成一帧一帧图片对应的pose;注需要ffmpeg
运行这个命令有
./scripts/extractAll.sh 171204_pose1_sample
171204_pose1_sample/hdImgs/00_00/00_00_00000000.jpg
171204_pose1_sample/hdPose3d_stage1_coco19/body3DScene_00000000.json

在这个工具里面有一些可视化的代码，以及从3D重投影到2D的方法
还有一些kinect数据点云的

注：https://github.com/wangzt-halo/das在DAS这个项目有一个代码可以将原始标签转化
</code></pre>
<p>480 views VGA video（标准视频）是指分辨率为 640x480 像素的视频。</p>
<ul>
<li>171204_pose1_sample&#x2F;hdVideos&#x2F;hd_00_XX.mp4  #synchronized HD video files (31 views)</li>
<li>171204_pose1_sample&#x2F;vgaVideos&#x2F;KINECTNODE%d&#x2F;vga_XX_XX.mp4 #synchrponized VGA video files (480 views)</li>
<li>171204_pose1_sample&#x2F;calibration_171204_pose1_sample.json #calibration files</li>
<li>171204_pose1_sample&#x2F;hdPose3d_stage1_coco19.tar #3D Body Keypoint Data (coco19 keypoint definition)</li>
<li>171204_pose1_sample&#x2F;hdFace3d.tar #3D Face Keypoint Data</li>
<li>171204_pose1_sample&#x2F;hdHand3d.tar #3D Hand Keypoint Data</li>
</ul>
<h2 id="COCO-keypoint数据"><a href="#COCO-keypoint数据" class="headerlink" title="COCO keypoint数据"></a>COCO keypoint数据</h2><p>150K训练5K验证30K测试</p>
<img src="../images/POSE/image-20230721142637523.png" alt="image-20230721142637523" style="zoom:80%;" />

<img src="../images/POSE/image-20230721142828338.png" alt="image-20230721142828338" style="zoom:50%;" />

<img src="../images/POSE/image-20230721142920961.png" alt="image-20230721142920961" style="zoom:80%;" />



<h2 id="MuPoTS"><a href="#MuPoTS" class="headerlink" title="MuPoTS"></a>MuPoTS</h2><p>POSENET组织格式</p>
<p>POSENET也有下载链接</p>
<pre><code>|   |-- MuPoTS
|   |   |-- bbox_root
|   |   |   |-- bbox_mupots_output.json
|   |   |-- data
|   |   |   |-- MultiPersonTestSet
|   |   |   |-- MuPoTS-3D.json
</code></pre>
<h2 id="MuCo"><a href="#MuCo" class="headerlink" title="MuCo"></a>MuCo</h2><p>POSENET组织格式</p>
<p>POSENET也有下载链接</p>
<pre><code>|   |-- MuCo
|   |   |-- data
|   |   |   |-- augmented_set
|   |   |   |-- unaugmented_set
|   |   |   |-- MuCo-3DHP.json
</code></pre>
<h2 id="Human-3-6使用"><a href="#Human-3-6使用" class="headerlink" title="Human 3.6使用"></a>Human 3.6使用</h2><p>原始数据下载</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42951560/article/details/126380971">https://blog.csdn.net/qq_42951560/article/details/126380971</a></p>
<h3 id="POSENET组织格式"><a href="#POSENET组织格式" class="headerlink" title="POSENET组织格式"></a>POSENET组织格式</h3><p>POSENET也有下载链接</p>
<p><a target="_blank" rel="noopener" href="https://github.com/mks0601/3DMPPE_POSENET_RELEASE">https://github.com/mks0601/3DMPPE_POSENET_RELEASE</a></p>
<pre><code>|   |-- Human36M
|   |   |-- bbox_root
|   |   |   |-- bbox_root_human36m_output.json
|   |   |-- images
|   |   |-- annotations
</code></pre>
<h3 id="RLE组织格式"><a href="#RLE组织格式" class="headerlink" title="RLE组织格式"></a>RLE组织格式</h3><pre><code>    |-- h36m
    `-- |-- annotations
        |   |-- Sample_trainmin_train_Human36M_protocol_2.json
        |   `-- Sample_64_test_Human36M_protocol_2.json
        `-- images
            |-- s_01_act_02_subact_01_ca_01
            |   |-- ...
            |-- s_01_act_02_subact_01_ca_02
            |   |-- ...
            `-- ... 
</code></pre>
<p>Human3.6是没有遮挡的数据的</p>
<p>18个关节点</p>
<p>Pixel-level 24 body part labels for each configuration</p>
<p>Time-of-flight range data 测距的数据</p>
<p>3D laser scans of the actors mesh数据</p>
<p><a target="_blank" rel="noopener" href="http://vision.imar.ro/human3.6m/description.php">http://vision.imar.ro/human3.6m/description.php</a></p>
<p>对于测距以及mesh的数据的描述</p>
<p>pose数据还包括relative 3D joint positions (R3DJP)和kinematic representation (KR)，KR用于描述物体的运动状态，例如位置、速度和加速度等。</p>
<img src="../images/POSE/image-20230721143257782.png" alt="image-20230721143257782" style="zoom:50%;" />

<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/h36m_annot.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/h36m_annot.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S1.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S1.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S5.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S5.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S6.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S6.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S7.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S7.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S8.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S8.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S9.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S9.tar</a></p>
<p><a target="_blank" rel="noopener" href="http://visiondata.cis.upenn.edu/volumetric/h36m/S11.tar">http://visiondata.cis.upenn.edu/volumetric/h36m/S11.tar</a></p>
<h1 id="学习RLE代码"><a href="#学习RLE代码" class="headerlink" title="学习RLE代码"></a>学习RLE代码</h1><h3 id="Train-on-MSCOCO"><a href="#Train-on-MSCOCO" class="headerlink" title="Train on MSCOCO"></a>Train on MSCOCO</h3><pre><code>./scripts/train.sh ./configs/256x192_res50_regress-flow.yaml train_rle_coco
</code></pre>
<p>train.sh里面的内容为：</p>
<pre><code>set -x
# 可以打开shell的调试模式，以便在shell执行时输出每个命令和其参数的执行结果。
CONFIG=$1 # 这将第一个命令行参数（$1）的值赋给名为CONFIG的变量。
EXPID=$&#123;2:-&quot;test_rle&quot;&#125; #  这将第二个命令行参数（$2）的值赋给名为EXPID的变量。如果第二个参数未提供，则						   #  EXPID的值将为&quot;test_rle&quot;。
PORT=$&#123;3:-23456&#125;
HOST=$(hostname -i)

python ./scripts/train.py --nThreads 16 \
    --launcher pytorch --rank 0 \
    --dist-url tcp://$&#123;HOST&#125;:$&#123;PORT&#125; \
    --exp-id $&#123;EXPID&#125; \
    --cfg $&#123;CONFIG&#125; --seed 123123
</code></pre>
<h3 id="写代码的技巧"><a href="#写代码的技巧" class="headerlink" title="写代码的技巧"></a>写代码的技巧</h3><h4 id="opt-snapshot"><a href="#opt-snapshot" class="headerlink" title="opt.snapshot"></a>opt.snapshot</h4><h4 id="‘pytorch’-‘slurm’-‘mpi’分布式平台"><a href="#‘pytorch’-‘slurm’-‘mpi’分布式平台" class="headerlink" title="‘pytorch’, ‘slurm’, ‘mpi’分布式平台"></a>‘pytorch’, ‘slurm’, ‘mpi’分布式平台</h4><h4 id="日志函数"><a href="#日志函数" class="headerlink" title="日志函数"></a>日志函数</h4><pre><code>from types import MethodType
import logging
logger = logging.getLogger(&#39;&#39;)
def epochInfo(self, set, idx, loss, acc):
    self.info(&#39;&#123;set&#125;-&#123;idx:d&#125; epoch | loss:&#123;loss:.8f&#125; | acc:&#123;acc:.4f&#125;&#39;.format(
        set=set,
        idx=idx,
        loss=loss,
        acc=acc
    ))
logger.epochInfo = MethodType(epochInfo, logger)

cfg_file_name = os.path.basename(opt.cfg)
filehandler = logging.FileHandler(&#39;./exp/&#123;&#125;-&#123;&#125;/training.log&#39;.format(opt.exp_id, cfg_file_name))
streamhandler = logging.StreamHandler()
logger.setLevel(logging.INFO)
logger.addHandler(filehandler)
logger.addHandler(streamhandler)
#打印配置文件
logger.info(&#39;******************************&#39;)
logger.info(opt)
logger.info(&#39;******************************&#39;)
logger.info(cfg)
logger.info(&#39;******************************&#39;)
</code></pre>
<h4 id="随机种子设置"><a href="#随机种子设置" class="headerlink" title="随机种子设置"></a>随机种子设置</h4><pre><code>def setup_seed(seed):
    random.seed(seed)
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False		#这样设置就行了
    torch.backends.cudnn.deterministic = True   #这样设置就行了
</code></pre>
<h4 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h4><h5 id="统计GPU个数"><a href="#统计GPU个数" class="headerlink" title="统计GPU个数"></a>统计GPU个数</h5><pre><code>torch.cuda.device_count()
</code></pre>
<h5 id="多进程并行运算"><a href="#多进程并行运算" class="headerlink" title="多进程并行运算"></a>多进程并行运算</h5><pre><code>import torch.multiprocessing as mp
ngpus_per_node = torch.cuda.device_count()
opt.ngpus_per_node = ngpus_per_node
mp.spawn(main_worker, nprocs=ngpus_per_node, args=(opt, cfg))
</code></pre>
<p>函数mp.spqwn</p>
<pre><code>mp.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method=&#39;spawn&#39;)
</code></pre>
<p>其中，参数的含义如下：</p>
<ul>
<li><code>fn</code>：要在每个子进程中执行的函数。</li>
<li><code>args</code>：传递给 <code>fn</code> 函数的参数。这应该是一个元组或列表。</li>
<li><code>nprocs</code>：要启动的子进程数。</li>
<li><code>join</code>：如果为 True，则主进程将等待所有子进程完成后再退出。如果为 False，则主进程不会等待子进程完成。</li>
<li><code>daemon</code>：如果为 True，则子进程将被设置为守护进程。这意味着它们将在主进程退出时自动终止。</li>
<li><code>start_method</code>：指定启动子进程的方法。可以是 ‘fork’ 或 ‘spawn’。默认值是 ‘spawn’，在 Windows 系统上只支持 ‘spawn’ 方法。</li>
</ul>
<p>使用 <code>mp.spawn</code> 函数，可以将模型训练任务分配给多个 GPU 或计算节点进行并行计算。每个子进程可以运行相同的代码，但是处理不同的数据块，从而实现模型训练的并行化。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h3><h4 id="SPPE"><a href="#SPPE" class="headerlink" title="SPPE"></a>SPPE</h4><p>rlepose&#x2F;models&#x2F;regression_nf.py</p>
<h5 id="RegressFlow"><a href="#RegressFlow" class="headerlink" title="RegressFlow"></a>RegressFlow</h5><pre><code>class RegressFlow(nn.Module):
    def __init__(self, norm_layer=nn.BatchNorm2d, **cfg):
        super(RegressFlow, self).__init__()
        self._preset_cfg = cfg[&#39;PRESET&#39;]
        self.fc_dim = cfg[&#39;NUM_FC_FILTERS&#39;]
        self._norm_layer = norm_layer
        self.num_joints = self._preset_cfg[&#39;NUM_JOINTS&#39;]
        self.height_dim = self._preset_cfg[&#39;IMAGE_SIZE&#39;][0]
        self.width_dim = self._preset_cfg[&#39;IMAGE_SIZE&#39;][1]

        self.preact = ResNet(f&quot;resnet&#123;cfg[&#39;NUM_LAYERS&#39;]&#125;&quot;)

        # Imagenet pretrain model
        import torchvision.models as tm  # noqa: F401,F403
        assert cfg[&#39;NUM_LAYERS&#39;] in [18, 34, 50, 101, 152]
        x = eval(f&quot;tm.resnet&#123;cfg[&#39;NUM_LAYERS&#39;]&#125;(pretrained=True)&quot;)

        self.feature_channel = &#123;
            18: 512,
            34: 512,
            50: 2048,
            101: 2048,
            152: 2048
        &#125;[cfg[&#39;NUM_LAYERS&#39;]]
        self.hidden_list = cfg[&#39;HIDDEN_LIST&#39;]

        model_state = self.preact.state_dict()
        state = &#123;k: v for k, v in x.state_dict().items()
                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()&#125;
        model_state.update(state)
        self.preact.load_state_dict(model_state)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        self.fcs, out_channel = self._make_fc_layer()

        self.fc_coord = Linear(out_channel, self.num_joints * 2)
        self.fc_sigma = Linear(out_channel, self.num_joints * 2, norm=False)

        self.fc_layers = [self.fc_coord, self.fc_sigma]

        prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))
        masks = torch.from_numpy(np.array([[0, 1], [1, 0]] * 3).astype(np.float32))

        self.flow = RealNVP(nets, nett, masks, prior)

    def _make_fc_layer(self):
        fc_layers = []
        num_deconv = len(self.fc_dim)
        input_channel = self.feature_channel
        for i in range(num_deconv):
            if self.fc_dim[i] &gt; 0:
                fc = nn.Linear(input_channel, self.fc_dim[i])
                bn = nn.BatchNorm1d(self.fc_dim[i])
                fc_layers.append(fc)
                fc_layers.append(bn)
                fc_layers.append(nn.ReLU(inplace=True))
                input_channel = self.fc_dim[i]
            else:
                fc_layers.append(nn.Identity())

        return nn.Sequential(*fc_layers), input_channel

    def _initialize(self):
        for m in self.fcs:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.01)
        for m in self.fc_layers:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.01)

    def forward(self, x, labels=None):
        BATCH_SIZE = x.shape[0]

        feat = self.preact(x)

        _, _, f_h, f_w = feat.shape
        feat = self.avg_pool(feat).reshape(BATCH_SIZE, -1)

        out_coord = self.fc_coord(feat).reshape(BATCH_SIZE, self.num_joints, 2)
        assert out_coord.shape[2] == 2

        out_sigma = self.fc_sigma(feat).reshape(BATCH_SIZE, self.num_joints, -1)

        # (B, N, 2)
        pred_jts = out_coord.reshape(BATCH_SIZE, self.num_joints, 2)
        sigma = out_sigma.reshape(BATCH_SIZE, self.num_joints, -1).sigmoid()
        scores = 1 - sigma

        scores = torch.mean(scores, dim=2, keepdim=True)

        if self.training and labels is not None:
            gt_uv = labels[&#39;target_uv&#39;].reshape(pred_jts.shape)
            bar_mu = (pred_jts - gt_uv) / sigma
            # (B, K, 2)
            log_phi = self.flow.log_prob(bar_mu.reshape(-1, 2)).reshape(BATCH_SIZE, self.num_joints, 1)

            nf_loss = torch.log(sigma) - log_phi
        else:
            nf_loss = None

        output = EasyDict(
            pred_jts=pred_jts,
            sigma=sigma,
            maxvals=scores.float(),
            nf_loss=nf_loss
        )
        return output
</code></pre>
<h5 id="RegressFlow3D"><a href="#RegressFlow3D" class="headerlink" title="RegressFlow3D"></a>RegressFlow3D</h5><pre><code>
class RegressFlow3D(nn.Module):
    def __init__(self, norm_layer=nn.BatchNorm2d, **cfg):
        super(RegressFlow3D, self).__init__()
        self._preset_cfg = cfg[&#39;PRESET&#39;]
        self.fc_dim = cfg[&#39;NUM_FC_FILTERS&#39;]
        self._norm_layer = norm_layer
        self.num_joints = self._preset_cfg[&#39;NUM_JOINTS&#39;]
        self.height_dim = self._preset_cfg[&#39;IMAGE_SIZE&#39;][0]
        self.width_dim = self._preset_cfg[&#39;IMAGE_SIZE&#39;][1]

        self.preact = ResNet(f&quot;resnet&#123;cfg[&#39;NUM_LAYERS&#39;]&#125;&quot;)

        # Imagenet pretrain model
        import torchvision.models as tm  # noqa: F401,F403
        assert cfg[&#39;NUM_LAYERS&#39;] in [18, 34, 50, 101, 152]
        x = eval(f&quot;tm.resnet&#123;cfg[&#39;NUM_LAYERS&#39;]&#125;(pretrained=True)&quot;)

        self.feature_channel = &#123;
            18: 512,
            34: 512,
            50: 2048,
            101: 2048
        &#125;[cfg[&#39;NUM_LAYERS&#39;]]

        self.root_idx = 0

        model_state = self.preact.state_dict()
        state = &#123;k: v for k, v in x.state_dict().items()
                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()&#125;
        model_state.update(state)
        self.preact.load_state_dict(model_state)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        self.fcs, out_channel = self._make_fc_layer()

        self.fc_coord = Linear(out_channel, self.num_joints * 3)
        self.fc_sigma = nn.Linear(out_channel, self.num_joints * 3)

        self.fc_layers = [self.fc_coord, self.fc_sigma]

        self.share_flow = True

        prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))
        masks = torch.from_numpy(np.array([[0, 1], [1, 0]] * 3).astype(np.float32))

        prior3d = distributions.MultivariateNormal(torch.zeros(3), torch.eye(3))
        masks3d = torch.from_numpy(np.array([[0, 0, 1], [1, 1, 0]] * 3).astype(np.float32))
        # masks3d = torch.from_numpy(np.array([[1, 1, 0], [0, 0, 1]] * 3).astype(np.float32))

        self.flow2d = RealNVP(nets, nett, masks, prior)
        self.flow3d = RealNVP(nets3d, nett3d, masks3d, prior3d)

    def _make_fc_layer(self):
        fc_layers = []
        num_deconv = len(self.fc_dim)
        input_channel = self.feature_channel
        for i in range(num_deconv):
            if self.fc_dim[i] &gt; 0:
                fc = nn.Linear(input_channel, self.fc_dim[i])
                bn = nn.BatchNorm1d(self.fc_dim[i])
                fc_layers.append(fc)
                fc_layers.append(bn)
                fc_layers.append(nn.ReLU(inplace=True))
                input_channel = self.fc_dim[i]
            else:
                fc_layers.append(nn.Identity())

        return nn.Sequential(*fc_layers), input_channel

    def _initialize(self):
        for m in self.fcs:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.01)
        for m in self.fc_layers:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.01)

    def forward(self, x, labels=None):
        BATCH_SIZE = x.shape[0]

        feat = self.preact(x)

        # Positional Pooling
        _, _, f_h, f_w = feat.shape
        feat = self.avg_pool(feat).reshape(BATCH_SIZE, -1)

        out_coord = self.fc_coord(feat).reshape(BATCH_SIZE, self.num_joints, 3)
        assert out_coord.shape[2] == 3

        out_sigma = self.fc_sigma(feat).reshape(BATCH_SIZE, self.num_joints, -1)

        # (B, N, 3)
        pred_jts = out_coord.reshape(BATCH_SIZE, self.num_joints, 3)
        if not self.training:
            pred_jts[:, :, 2] = pred_jts[:, :, 2] - pred_jts[:, self.root_idx:self.root_idx + 1, 2]

        sigma = out_sigma.reshape(BATCH_SIZE, self.num_joints, -1).sigmoid() + 1e-9
        scores = 1 - sigma

        scores = torch.mean(scores, dim=2, keepdim=True)

        if labels is not None:
            gt_uvd = labels[&#39;target_uvd&#39;].reshape(pred_jts.shape)
            gt_uvd_weight = labels[&#39;target_uvd_weight&#39;].reshape(pred_jts.shape)
            gt_3d_mask = gt_uvd_weight[:, :, 2].reshape(-1)

            assert pred_jts.shape == sigma.shape, (pred_jts.shape, sigma.shape)
            bar_mu = (pred_jts - gt_uvd) / sigma
            bar_mu = bar_mu.reshape(-1, 3)
            bar_mu_3d = bar_mu[gt_3d_mask &gt; 0]
            bar_mu_2d = bar_mu[gt_3d_mask &lt; 1][:, :2]
            # (B, K, 3)
            log_phi_3d = self.flow3d.log_prob(bar_mu_3d)
            log_phi_2d = self.flow2d.log_prob(bar_mu_2d)
            log_phi = torch.zeros_like(bar_mu[:, 0])
            log_phi[gt_3d_mask &gt; 0] = log_phi_3d
            log_phi[gt_3d_mask &lt; 1] = log_phi_2d
            log_phi = log_phi.reshape(BATCH_SIZE, self.num_joints, 1)

            nf_loss = torch.log(sigma) - log_phi
        else:
            nf_loss = None

        output = EasyDict(
            pred_jts=pred_jts,
            sigma=sigma,
            maxvals=scores.float(),
            nf_loss=nf_loss
        )
        return output
</code></pre>

    </div>

    <div class="post-meta">
        <i>
        
            <span>2023-07-13</span>
            
                <span>该篇文章被 hong</span>
            
            
                <span>打上标签:
                    
                    
                        <a href='/tags/pose%E8%AE%BA%E6%96%87/'>
                            pose论文
                        </a>
                    
                </span>
             
             
        
        </i>
    </div>
    
        

     
</div>



                    
                    
                    <div class="footer">
    
        <span> 
             

            
                

            
        </span>
    
</div>
<!--这是指一条线往下的内容-->
<div class="footer-last">
    
            <span>花有重开日，人无再少年！</span>
            
                <span class="footer-last-span-right"><i>本站由<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>驱动｜使用<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>主题</i></span>
            
    
</div>


    
        
<link rel="stylesheet" href="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/css/a11y-dark.min.css">

        
<script src="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/js/highlight.min.js"></script>

        
<script src="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/js/highlightjs-line-numbers.js"></script>

    


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>
                </div>
            
    </body>
</html>